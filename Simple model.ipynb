{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "8898041\n",
      "Reading word_vectors... 100000/400000\n",
      "Reading word_vectors... 200000/400000\n",
      "Reading word_vectors... 300000/400000\n",
      "Reading word_vectors... 400000/400000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "word_vectors = {}\n",
    "\n",
    "glove_size = 0\n",
    "with open('data/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        glove_size+=1\n",
    "print glove_size\n",
    "\n",
    "review_size = 0\n",
    "with open('data/reviews_Books_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        review_size+=1\n",
    "print review_size\n",
    "\n",
    "line_index = 0\n",
    "with open('data/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line_index+=1\n",
    "        if line_index%100000==0:\n",
    "            print \"Reading word_vectors... \"+str(line_index)+\"/\"+str(glove_size)\n",
    "        line_list = line.split()\n",
    "        word = line_list[0]\n",
    "        vec = [float(v) for v in line_list[1:]]\n",
    "        assert len(vec) == 100\n",
    "        word_vectors[word] = vec\n",
    "        \n",
    "print len(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid reviews: 48933\n",
      "Valid reviews: 97870\n",
      "Valid reviews: 146819\n",
      "Valid reviews: 195769\n",
      "Valid reviews: 244603\n",
      "Valid reviews: 293552\n",
      "Valid reviews: 342548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-085e469ecfd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfinal_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "line_index = 0\n",
    "with open('data/reviews_Books_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        line_index+=1\n",
    "        if line_index%100000==0:\n",
    "#             print \"Reading word_vectors... \"+str(line_index)+\"/\"+str(review_size)\n",
    "            print \"Valid reviews: \"+str(len(y_data))\n",
    "        if \"reviewText\" not in line or \"overall\" not in line:\n",
    "            continue\n",
    "        review_start = line.index(\"\\\"reviewText\\\":\")+15\n",
    "        review_end = line.index(\"\\\"overall\\\":\")-3\n",
    "        if review_end-review_start<50:\n",
    "            continue\n",
    "        review = line[review_start:review_end]\n",
    "        overall = line[review_end+14:review_end+15]\n",
    "        \n",
    "        for i in xrange(len(review)):\n",
    "            if not review[i].isalpha():\n",
    "                review = review[:i]+\" \"+review[i:]\n",
    "        review = review.split()\n",
    "        final_vec = np.zeros(100)\n",
    "        n_skip = 0\n",
    "        for word in review:\n",
    "            word_l = word.lower()\n",
    "            if word_l not in word_vectors:\n",
    "                n_skip += 1\n",
    "            else:\n",
    "                final_vec+=np.array(word_vectors[word_l])\n",
    "        if n_skip>0.25*len(review):\n",
    "            continue\n",
    "        else:\n",
    "            final_vec/=float(len(review)-n_skip)\n",
    "            overall = int(overall)\n",
    "            x_data.append(final_vec)\n",
    "            y_data.append(overall)\n",
    "\n",
    "# print \"Reading word_vectors... \"+str(line_index)+\"/\"+str(review_size)\n",
    "print \"Valid reviews: \"+str(len(y_data))\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "y_onehot = np.zeros((len(y_data), 5))\n",
    "y_onehot[np.arange(len(y_data)), y_data-1] = 1\n",
    "\n",
    "print x_data.shape\n",
    "print y_data.shape\n",
    "print y_onehot.shape\n",
    "print \"-------------\"\n",
    "print x_data[:5]\n",
    "print y_onehot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stats = np.sum(y_onehot, axis = 0)\n",
    "print y_stats\n",
    "num_in_class = min(y_stats)\n",
    "new_x = []\n",
    "new_y = []\n",
    "new_y_onehot = []\n",
    "nums = [0,0,0,0,0]\n",
    "for i in xrange(len(y_data)):\n",
    "    index = y_data[i]\n",
    "    if nums[index]<num_in_class:\n",
    "        new_x.append(x_data[i])\n",
    "        new_y.append(y_data[i])\n",
    "        new_y_onehot.append(y_onehot[i])\n",
    "x_data = new_x\n",
    "y_data = new_y\n",
    "y_onehot = new_y_onehot\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "y_onehot = np.array(y_onehot)\n",
    "y_stats = np.sum(y_onehot, axis = 0)\n",
    "print y_stats\n",
    "np.save(\"x_data\", x_data)\n",
    "np.save(\"y_onehot\", y_onehot)\n",
    "np.save(\"y_data\", y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "epsilon = 0.000001\n",
    "\n",
    "x_data = np.load('x_data.npy')\n",
    "y_onehot = np.load('y_onehot.npy')\n",
    "y_data = np.load('y_data.npy')\n",
    "\n",
    "x_train = x_data[:400000]\n",
    "y_train = y_onehot[:400000]\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "xs = tf.placeholder(tf.float32, [None, 100])\n",
    "ys = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=0., stddev=1.))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "    fc_mean, fc_var = tf.nn.moments(Wx_plus_b,axes=[0],)\n",
    "    scale = tf.Variable(tf.ones([out_size]))\n",
    "    shift = tf.Variable(tf.zeros([out_size]))\n",
    "    Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "l1 = add_layer(xs, 100, 200, activation_function=tf.nn.relu)\n",
    "l2 = add_layer(l1, 200, 200, activation_function=tf.nn.relu)\n",
    "output = add_layer(l2, 200, 5, activation_function=tf.nn.relu)\n",
    "output = tf.nn.softmax(output)\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(output), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    _xs = sess.run(xs, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    _l1 = sess.run(l1, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    output_ = sess.run(output, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    loss_value = sess.run(loss, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    print _xs\n",
    "    print _l1\n",
    "    print loss_value\n",
    "    print output_\n",
    "    print \"------------------------\"\n",
    "    sess.run(train_step, feed_dict={xs: x_train[j*100:(j+1)*100], ys: y_train[j*100:(j+1)*100], keep_prob: 0.5})\n",
    "    _xs = sess.run(xs, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    _l1 = sess.run(l1, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    output_ = sess.run(output, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    loss_value = sess.run(loss, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    print _xs\n",
    "    print _l1\n",
    "    print loss_value\n",
    "    print output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.2\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in xrange(30):\n",
    "    for j in xrange(4000):\n",
    "        sess.run(train_step, feed_dict={xs: x_train[j*100:(j+1)*100], ys: y_train[j*100:(j+1)*100], keep_prob: 0.5})\n",
    "    ys_ = sess.run(ys, feed_dict={xs: x_train[:100], ys: y_train[:100], keep_prob: 1})\n",
    "    output_ = sess.run(output, feed_dict={xs: x_train, ys: y_train, keep_prob: 1})\n",
    "    loss_value = sess.run(loss, feed_dict={xs: x_train, ys: y_train, keep_prob: 1})\n",
    "#         print \"Round \"+str(i)+\": Batch \"+str(j)+\" -- \"+str(ys_)\n",
    "#         print \"Round \"+str(i)+\": Batch \"+str(j)+\" -- \"+str(output_)\n",
    "    pred = np.argmax(output_, axis = 1)\n",
    "    a = 0\n",
    "    for ind in xrange(len(pred)):\n",
    "        if pred[ind] == y_data[ind]:\n",
    "            a+=1\n",
    "    correct_ratio = float(a)/len(pred)\n",
    "    print \"Round \"+str(i)+\" loss: \"+str(loss_value)+\", correct_ratio \"+\" -- \"+str(correct_ratio)\n",
    "    LR*=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_data[400000:]\n",
    "y_test = y_data[400000:]\n",
    "yh_test = y_onehot[400000:]\n",
    "print x_test.shape\n",
    "print y_test.shape\n",
    "print yh_test.shape\n",
    "output_ = sess.run(output, feed_dict={xs: x_test, ys: yh_test, keep_prob: 1})\n",
    "pred = np.argmax(output_, axis = 1)\n",
    "a = 0\n",
    "for ind in xrange(len(pred)):\n",
    "    if pred[ind] == y_test[ind]:\n",
    "        a+=1\n",
    "correct_ratio = float(a)/len(pred)\n",
    "print \"correct_ratio \"+\" -- \"+str(correct_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load('x_data.npy')\n",
    "y_onehot = np.load('y_onehot.npy')\n",
    "y_data = np.load('y_data.npy')\n",
    "\n",
    "x_train = x_data[:400000]\n",
    "y_train = y_onehot[:400000]\n",
    "x_test = x_data[400000:]\n",
    "y_test = y_data[400000:]\n",
    "yh_test = y_onehot[400000:]\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print \"------------\"\n",
    "print x_test.shape\n",
    "print y_test.shape\n",
    "print yh_test.shape\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "print \"training...\"\n",
    "neigh.fit(x_train[:5000], y_data[:5000])\n",
    "pred = neigh.predict(x_test)\n",
    "print pred.shape\n",
    "a = 0\n",
    "for ind in xrange(len(pred)):\n",
    "    if pred[ind] == y_test[ind]:\n",
    "        a+=1\n",
    "correct_ratio = float(a)/len(pred)\n",
    "print \"correct_ratio \"+\" -- \"+str(correct_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "print \"training...\"\n",
    "neigh.fit(x_train[:5000], y_data[:5000])\n",
    "pred = neigh.predict(x_test)\n",
    "print pred.shape\n",
    "a = 0\n",
    "for ind in xrange(len(pred)):\n",
    "    if pred[ind] == y_test[ind]:\n",
    "        a+=1\n",
    "correct_ratio = float(a)/len(pred)\n",
    "print \"correct_ratio \"+\" -- \"+str(correct_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "print \"training...\"\n",
    "neigh.fit(x_train[:5000], y_data[:5000])\n",
    "pred = neigh.predict(x_test)\n",
    "print pred.shape\n",
    "a = 0\n",
    "for ind in xrange(len(pred)):\n",
    "    if pred[ind] == y_test[ind]:\n",
    "        a+=1\n",
    "correct_ratio = float(a)/len(pred)\n",
    "print \"correct_ratio \"+\" -- \"+str(correct_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(x_train[:10000], y_data[:10000])\n",
    "pred = lin_clf.decision_function(x_test)\n",
    "pred = np.argmax(pred, axis = 1)\n",
    "print pred.shape\n",
    "a = 0\n",
    "for ind in xrange(len(pred)):\n",
    "    if pred[ind] == y_test[ind]:\n",
    "        a+=1\n",
    "correct_ratio = float(a)/len(pred)\n",
    "print \"correct_ratio \"+\" -- \"+str(correct_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "padding_vec = np.zeros(100)\n",
    "\n",
    "line_index = 0\n",
    "with open('data/reviews_Books_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        line_index+=1\n",
    "        if line_index%100000==0:\n",
    "#             print \"Reading word_vectors... \"+str(line_index)+\"/\"+str(review_size)\n",
    "            print \"Valid reviews: \"+str(len(y_data))\n",
    "        if \"reviewText\" not in line or \"overall\" not in line:\n",
    "            continue\n",
    "        review_start = line.index(\"\\\"reviewText\\\":\")+15\n",
    "        review_end = line.index(\"\\\"overall\\\":\")-3\n",
    "        if review_end-review_start<50:\n",
    "            continue\n",
    "        review = line[review_start:review_end]\n",
    "        overall = line[review_end+14:review_end+15]\n",
    "        \n",
    "        for i in xrange(len(review)):\n",
    "            if not review[i].isalpha():\n",
    "                review = review[:i]+\" \"+review[i:]\n",
    "        review = review.split()\n",
    "        \n",
    "        word_sequence = []\n",
    "        for i in xrange(50):\n",
    "            if i>=len(review):\n",
    "                word_vec = padding_vec\n",
    "            else:\n",
    "                word = review[i].lower()\n",
    "                if word in word_vectors:\n",
    "                    word_vec = np.array(word_vectors[word])\n",
    "                else:\n",
    "                    word_vec = np.random.rand(100)\n",
    "            word_sequence.append(word_vec)\n",
    "        x_data.append(word_sequence)\n",
    "        overall = int(overall)\n",
    "        y_data.append(overall)\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "x_data = np.transpose(x_data,(1,0,2))\n",
    "print x_data.shape\n",
    "print y_data.shape\n",
    "y_onehot = np.zeros((len(y_data), 5))\n",
    "y_onehot[np.arange(len(y_data)), y_data-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_stats = np.sum(y_onehot, axis = 0)\n",
    "print y_stats\n",
    "num_in_class = min(y_stats)\n",
    "new_x = []\n",
    "new_y = []\n",
    "new_y_onehot = []\n",
    "nums = [0,0,0,0,0]\n",
    "for i in xrange(len(y_data)):\n",
    "    index = y_data[i]\n",
    "    if nums[index]<num_in_class:\n",
    "        new_x.append(x_data[i])\n",
    "        new_y.append(y_data[i])\n",
    "        new_y_onehot.append(y_onehot[i])\n",
    "x_data = new_x\n",
    "y_data = new_y\n",
    "y_onehot = new_y_onehot\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "y_onehot = np.array(y_onehot)\n",
    "y_stats = np.sum(y_onehot, axis = 0)\n",
    "print y_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"x_data_lstm\", x_data)\n",
    "np.save(\"y_onehot_lstm\", y_onehot)\n",
    "np.save(\"y_data_lstm\", y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "x_data = np.load('x_data_lstm.npy')\n",
    "y_onehot = np.load('y_onehot_lstm.npy')\n",
    "y_data = np.load('y_data_lstm.npy')\n",
    "\n",
    "x_train = x_data[:,:90000,:]\n",
    "y_train = y_onehot[:90000]\n",
    "x_test = x_data[:,90000:,:]\n",
    "y_test = y_onehot[90000:]\n",
    "\n",
    "LR = 0.1\n",
    "alpha = 0.99\n",
    "epsilon = 0.000001\n",
    "hidden = 64\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "xs = tf.placeholder(tf.float32, [50, None, 100])\n",
    "ys = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=0., stddev=1.))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "    fc_mean, fc_var = tf.nn.moments(Wx_plus_b,axes=[0],)\n",
    "    scale = tf.Variable(tf.ones([out_size]))\n",
    "    shift = tf.Variable(tf.zeros([out_size]))\n",
    "    Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, fc_mean, fc_var, shift, scale, epsilon)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "fw_cells = [rnn.BasicLSTMCell(hidden,state_is_tuple=True)]\n",
    "bw_cells = [rnn.BasicLSTMCell(hidden,state_is_tuple=True)]\n",
    "# xs_unstack = tf.unstack(xs,axis=0)\n",
    "output_bi, output_state_fw, output_state_bw = rnn.stack_bidirectional_dynamic_rnn(fw_cells,bw_cells,xs,dtype=tf.float32,time_major=True)\n",
    "features = tf.concat([output_state_fw[0][0], output_state_fw[0][1], output_state_bw[0][0], output_state_bw[0][1]], 1)\n",
    "\n",
    "l1 = add_layer(features, 256, 500, activation_function=tf.nn.relu)\n",
    "l2 = add_layer(l1, 500, 500, activation_function=tf.nn.relu)\n",
    "l3 = add_layer(l2, 500, 5, activation_function=tf.nn.relu)\n",
    "output = tf.nn.softmax(l3)\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(output), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    _xs = sess.run(xs, feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "    _output_fw = sess.run(output_state_fw[0], feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "    _output_bw = sess.run(output_state_bw[0], feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "    print _xs.shape\n",
    "    print _output_fw[0].shape\n",
    "    print _output_fw[1].shape\n",
    "    print _output_bw[0].shape\n",
    "    print _output_bw[1].shape\n",
    "    _output = sess.run(output_bi, feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "    print type(_output)\n",
    "    print _output.shape\n",
    "    _feature = sess.run(features, feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "    print _feature.shape\n",
    "\"\"\"\n",
    "print \"Finished initialization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "alpha = 0.99\n",
    "epsilon = 0.000001\n",
    "hidden = 64\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in xrange(30):\n",
    "        for j in xrange(900):\n",
    "            sess.run(train_step, feed_dict={xs: x_train[:, j*100:(j+1)*100, :], ys: y_train[j*100:(j+1)*100, :], keep_prob: 0.5})\n",
    "            if (j+1)%100 == 0:\n",
    "                print \"training... \"+str(j+1)+\"/900\"\n",
    "        _output = sess.run(output, feed_dict={xs: x_train[:, :100, :], ys: y_train[:100], keep_prob: 1})\n",
    "        print _output.shape\n",
    "        print _output\n",
    "#         output_ = sess.run(output, feed_dict={xs: x_train, ys: y_train, keep_prob: 1})\n",
    "#         pred = np.argmax(output_, axis = 1)\n",
    "#         a = 0\n",
    "#         for ind in xrange(len(pred)):\n",
    "#             if pred[ind] == y_data[ind]:\n",
    "#                 a+=1\n",
    "#         correct_ratio = float(a)/len(pred)\n",
    "#         print \"Round \"+str(i)+\", correct_ratio \"+\" -- \"+str(correct_ratio)\n",
    "        LR*=alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
